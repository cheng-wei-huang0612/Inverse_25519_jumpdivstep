        mov x11, x12                           // ............................*............................
        add x12, x12, #1048576                 // ............................*............................
        add x10, x9, x6                        // .............................*...........................
        add x11, x11, #1048576                 // .............................*...........................
        lsl x12, x12, #22                      // ..............................*..........................
        csneg x3, x3, x13, pl                  // ..............................*..........................
        add v16.4S, v10.4S, v16.4S             // ...............................*.........................
        add x11, x11, x0                       // ...............................*.........................
        asr x13, x10, #1                       // ...............................*.........................
        asr x6, x12, #43                       // ................................*........................
        asr x9, x11, #42                       // .................................*.......................
        and v10.16B, v16.16B, v8.16B           // ..................................*......................
        sshr v16.4S, v16.4S, #30               // ..................................*......................
        smulh x10, x6, x4                      // ..................................*......................
        mul x7, x6, x4                         // ...................................*.....................
        mul x11, x9, x5                        // ...................................*.....................
        smulh x12, x9, x5                      // ...................................*.....................
        ushr v16.2D, v16.2D, #32               // .....................................*...................
        adds x11, x7, x11                      // ......................................*..................
        mov x7, x13                            // ......................................*..................
        add x13, x13, #1048576                 // .......................................*.................
        add x7, x7, #1048576                   // .......................................*.................
        add v11.4S, v11.4S, v16.4S             // ........................................*................
        adc x12, x10, x12                      // .........................................*...............
        add x10, x7, x0                        // .........................................*...............
        lsl x7, x13, #22                       // .........................................*...............
        lsl x13, x12, #44                      // ..........................................*..............
        lsr x11, x11, #20                      // ...........................................*.............
        asr x7, x7, #43                        // ...........................................*.............
        asr x10, x10, #42                      // ...........................................*.............
        orr x11, x11, x13                      // ............................................*............
        mul x13, x10, x5                       // .............................................*...........
        smulh x12, x7, x4                      // .............................................*...........
        smulh x5, x10, x5                      // .............................................*...........
        mul x4, x7, x4                         // ..............................................*..........
        adds x4, x4, x13                       // ..................................................*......
        adc x12, x12, x5                       // ...................................................*.....
        lsr x4, x4, #20                        // ....................................................*....
        mov x5, x11                            // ....................................................*....
        lsl x11, x12, #44                      // ....................................................*....
        and x12, x5, #1048575                  // .....................................................*...
        orr x4, x4, x11                        // ......................................................*..
        sub x11, x12, x0                       // .......................................................*.
        and x12, x4, #1048575                  // .......................................................*.
        sub x12, x12, x2                       // ........................................................*



# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#14,<m=int64#4,#1
# asm 2: sub >m1=x13,<m=x3,#1
sub x13,x3,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#13, #1
# asm 2: tst <grs=x12, #1
tst x12, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#15, <fuv=int64#12, xzr, ne
# asm 2: csel >ff=x14, <fuv=x11, xzr, ne
csel x14, x11, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#14, <grs=int64#13, ROR #1
# asm 2: tst <m1=x13, <grs=x12, ROR #1
tst x13, x12, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#4, <m1=int64#14, <m=int64#4, pl
# asm 2: csneg >m=x3, <m1=x13, <m=x3, pl
csneg x3, x13, x3, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#12, <grs=int64#13, <fuv=int64#12, mi
# asm 2: csel >fuv=x11, <grs=x12, <fuv=x11, mi
csel x11, x12, x11, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#15, <ff=int64#15, <ff=int64#15, pl
# asm 2: csneg >ff=x14, <ff=x14, <ff=x14, pl
csneg x14, x14, x14, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#13,<grs=int64#13,<ff=int64#15
# asm 2: add >grs=x12,<grs=x12,<ff=x14
add x12,x12,x14

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#13, <grs=int64#13, #1
# asm 2: asr >grs=x12, <grs=x12, #1
asr x12, x12, #1

# qhasm:     free m1

# qhasm:     free ff

# qhasm:     m1 = m - 1 
# asm 1: sub >m1=int64#14,<m=int64#4,#1
# asm 2: sub >m1=x13,<m=x3,#1
sub x13,x3,#1

# qhasm:     grs & 1
# asm 1: tst <grs=int64#13, #1
# asm 2: tst <grs=x12, #1
tst x12, #1

# qhasm:     ff = fuv if Z=0 else 0
# asm 1: csel >ff=int64#15, <fuv=int64#12, xzr, ne
# asm 2: csel >ff=x14, <fuv=x11, xzr, ne
csel x14, x11, xzr, ne

# qhasm:     m1 & (grs >>> 1)
# asm 1: tst <m1=int64#14, <grs=int64#13, ROR #1
# asm 2: tst <m1=x13, <grs=x12, ROR #1
tst x13, x12, ROR #1

# qhasm:     m = m1 if N=0 else -m
# asm 1: csneg >m=int64#4, <m1=int64#14, <m=int64#4, pl
# asm 2: csneg >m=x3, <m1=x13, <m=x3, pl
csneg x3, x13, x3, pl

# qhasm:     fuv = grs if N=1 else fuv
# asm 1: csel >fuv=int64#12, <grs=int64#13, <fuv=int64#12, mi
# asm 2: csel >fuv=x11, <grs=x12, <fuv=x11, mi
csel x11, x12, x11, mi

# qhasm:     ff = ff if N=0 else -ff
# asm 1: csneg >ff=int64#15, <ff=int64#15, <ff=int64#15, pl
# asm 2: csneg >ff=x14, <ff=x14, <ff=x14, pl
csneg x14, x14, x14, pl

# qhasm:     grs = grs + ff
# asm 1: add >grs=int64#13,<grs=int64#13,<ff=int64#15
# asm 2: add >grs=x12,<grs=x12,<ff=x14
add x12,x12,x14

# qhasm:     grs = grs signed>> 1
# asm 1: asr >grs=int64#13, <grs=int64#13, #1
# asm 2: asr >grs=x12, <grs=x12, #1
asr x12, x12, #1

# qhasm:     free m1

# qhasm:     free ff